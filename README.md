# Detecting-Misinformation-and-Fake-News-Titles-Online

CS 4100 Final Course Project
December 8, 2025
Professor Zhang
Group Members: Keira Lowden, Brandon Gunasti, Jack Roberge, Shanie Hurvitz

Abstract: 

We addressed the problem of the spread of misinformation by developing an AI agent to classify news titles as true or fake. Our approach employed a retrieval-augmented reasoning agent utilizing a TF-IDF search backend and a small Large Language Model for generating interpretable predictions. Out of 100 predictions, the model made 62 valid predictions, achieving an overall accuracy of 46.77% on this subset. While these do indicate some challenges in the model's performance, overall, there were several important conclusions from this accuracy experiment and the model overall. 

Overview: 

The key problem is the difficulty in combating the sheer volume and speed with which misinformation spreads quickly online. Many users, particularly older adults and those with lower media literacy overall, struggle to tell real vs. fake stories. This is often exploited by fake headlines that use emotional or exaggerated language to attract clicks, likes, and viewership. The extremely negative effects of disinformation are relatively uncontroversial, including erosion of trust, increased polarization, and even political and social violence. Our approach here seeks to get at solving a bigger issue in society, which is the fact that the truth has become so objective to one's beliefs, and we often struggle to arrive at mainstream truths. As a society, when we are unable to agree on an objective truth or foundation for discussion, it becomes difficult to find any common ground. Furthermore, AI is often discussed through the lenses of its economic and technological benefits as well as its social detriments, but few understand or believe that it can have a genuinely positive effect on societ cohesion and community. Our project seeks to highlight the possibility for finding common ground for discussion.

We are proposing using a retrieval-augmented classification system built around the ReAct (Thought -> Action -> Observation) loops to try to classify news headlines as either fake or true. This architecture uses a retrieval-augmented reasoning agent with a TF-IDF search method paired with a small LLM. The goal is to classify news titles automatically, while also providing some sort of reasoning and/or logic to the model's response to help the user understand the context to our agent's answer. The rationale was to leverage the interpretable step-by-step reasoning capabilities of an LLM while grounding its decisions with evidence retrieved via TF-IDF search. While the TF-IDF approach to classifying misinformation actually seems to be relatively common from research, our unique contribution was integrating this common approach into an LLM-based agent pipeline, which allows for complex decision logic via the ReAct schema that a simple TF-IDF algorithm may not be able to achieve. We also decided to allow the model to abstain from prediction in order to improve reliability by avoiding low-confidence guesses. While this decision may impact the usefulness and final performance of the model, we felt it was important to not "box the model in" to making a decision, and give it the breathing room to abstain from a choice. The key technical components of the model the small instruction-tuned LLM (Qwen 0.5B/1.5B) and the TF-IDF backend. Our experimental results show an accuracy of 46.77% on valid predictions (i.e., the ones the model doesn't abstain from). We found the project was hindered by a few specific limitations. A major limitation encountered was the sensitivity to prompt format and the general limited model capacity of the small LLM used.

Approach: 

The overall prediction process is governed by a Retrieval-Augmented classification algorithm, which is executed via a ReAct loop that's run by the small instruction-tuned Qwen 0.5B/1.5B LLM. When presented with a new news title that needs classification (fake/true), the LLM first generates a Thought (its reasoning) and then an Action, which is always a call to our custom search tool. This search tool is the TF-IDF backend; it then converts the input title into a vector using TF-IDF vectorization. From here, the search tool queries the corpus to retrieve the top k=5 most similar, already-labeled headlines using cosine-similarity ranking. These retrieved results are returned as the Observation and fed back into the LLM's context. The LLM then integrates this external evidence to generate its next Thought and decides to either continue the process for up to six steps or output the final classification (True, Fake, or Abstain). We make a few key assumptions and design choices in this approach. First, a key tenet of our project is the assumption that news titles actually contain enough information to make a determination as to whether the article is fake or true. This assumption is central to the project, as if this were not the case, more information (say the first few sentences of the article, for example) would be required for each input to generate a valid output. We also decided to allow the model to abstain from choosing fake or true when classifying a news model. This allows the model to ignore the situations where it is the most unsure (often likely the toughest calls), meaning the model will only make a choice when it is relatively sure of it's answer. This will help to improve the accuracy and trustworthiness of the agent, but also means that its utility will be lower given the many headlines it will not classify at all. Furthermore, for many missed headlines, the agent may have actually been correct to go with its intuition, but with an option to abstain, it may "chicken out" of answering a more difficult situation. Additional limitations include the model's capacity and our own compute power. Because we are using a relatively small LLM there is a limit to how much deep learning can happen in using the model in our agent pipeline. Furthermore, given we are working on home computers, many of us do not have on aggregate the computation for expansive testing or more intense learning & training. Lastly, our approach comes with a real sensitivity to the prompt format for the model; given the ReAct format we are using, the LLM must adhere to a strict syntax when parsing thoughts, potentially resulting in possibly invalid or unparseable outputs.  

Experiments:

Our dataset is from WELFake and includes approximately 72,000 samples with a roughly equal binary label distribution (T/F) and news headlines covering a wide topical variety of subjects. The project uses a Retrieval-augmented agent pipeline. The LLM generates Thought/Action pairs, the TF-IDF backend performs the search, and the agent aggregates the tools to produce the final label. The model we use is a Qwen 0.5B/1.5B. We set our TF-IDF search depth to k=5, meaning that the TF-IDF search will pull the top 5 most similar (according to cosine similarity) results when searching for relevant, already-labelled headlines. We set new tokens a maximum of 120 to ensure efficiency and prevent hallucination by the LLM. Lastly, we include limit the model to 6 reasoning steps to ensure it arrives at an output reasonably, and also to prevent excessive rumination, whilst still allowing time for adequate reasoning and thought. 

Results:

From our experiment, out of 100 samples tested, only 62 were valid predictions (True/Fake), with 38 invalid/unparsed predictions. The accuracy of the 62 valid predictions was 46.77%. In our confusion matrix, we see the model correctly labeled 21 true headlines as true, mislabeled 7 true headlines as fake, mislabeled 26 fake headlines as true, and correctly labeled 8 fake headlines as fake. We further see that for the True class, the model had a precision of 0.45, a recall of 0.75, and an f1-score of 0.56. For the fake class, the agent had a precision of 0.53, a recall of 0.24, and f1-score of 0.33. These results show that while the model has a higher recall for true articles (0.75), it has very low precision (0.45), meaning it frequently misclassifies fake articles as true. Conversely, the recall for fake articles is very low (0.24), indicating it struggles significantly to identify misinformation.

In terms of the chosen parameters, k = 5 was chosen for search depth and max_steps = 6 to keep the reasoning process controlled and computationally manageable. These limits help reduce hallucinated intermediate actions and ensure the ReAct loops remain consistent and stable throughout the reasoning steps.

Discussion:

From the results of our experiment, we see that the model struggled to effectively differentiate misinformation. In diagnosing the agent's shortcomings, we can draw a few conclusions. The first conclusion is that the headline is likely not enough information for the model to conclusively determine whether an article is fake news or not. This is not surprising, as many headlines are not immediately apparent as true or false, and more information or context is often required. We can also conclude that the limited capacity of our LLM and compute power was a potential issue, as the model was likely not strong enough to handle the complex ReAct reasoning process on a large scale, which complicated random sample testing. Running analysis on a broader or full dataset was impossible because of the time required to classify each sample with the power of our home computers, in addition to the fact that progress would sometimes halt before completing the analysis. The limited and statistically insignificant dataset we were able to use likely skewed our performance metrics and rendered our results non-representative of the model’s full capability.

Furthermore, we found that the presence of contradictory or misleading information online, of which there is a significant amount, often confused the LLM. From the reasoning chains, we could see that the model was often able to correctly conclude that a true article was true, since there is usually abundant supporting information online. However, it struggled with misinformation because there is often at least some content (tweets, Reddit threads, YouTube videos, and similar sources) that supports or echoes the false claim, and this was enough to confuse the model and prevent a confident conclusion. As a result, the model was more likely to abstain when confronted with uncertain or noisy information. Nevertheless, when examining the model’s reasoning process, we found that its thought patterns were often on the right track. True headlines were frequently classified correctly with sound logic. In contrast, false headlines tended to introduce substantial uncertainty, pushing the model to search deeper for evidence and often leading it to results that increased confusion.

Conclusion:

In this project, we built a retrieval augmented reasoning agent that uses TF IDF search and a small instruction tuned LLM to classify news headlines as true or fake. Although the model struggled with both overall accuracy and the challenge of working only from headlines, the system still provided valuable insight into how lightweight reasoning agents perform when given limited context and confronted with noisy information. Our results show that small models often lack the capacity needed for reliable decision making in misinformation detection and are highly sensitive to prompt structure and retrieval quality. Even with these limitations, the project demonstrates a complete end to end agent pipeline and clarifies which areas require improvement. With richer context, more powerful models, and more advanced retrieval methods, this approach could become significantly more effective in future work.

References: 

Shahane, Saurabh. “Fake News Classification.” Kaggle, 8 Oct. 2023, www.kaggle.com/datasets/saurabhshahane/fake-news-classification. 

