# Detecting-Misinformation-and-Fake-News-Titles-Online

Abstract: 

We addressed the problem of the spread of misinformation by developing an AI agent to classify news titles as true or fake. Our approach employed a retrieval-augmented reasoning agent utilizing a TF-IDF search backend and a small Large Language Model for generating interpretable predictions. Out of 100 predictions, the model made 62 valid predictions, achieving an overall accuracy of 46.77% on this subset. While these do indicate some challenges in the model's performance, overall, there were several important conclusions from this accuracy experiment and the model overall. 

Overview: Summarize your project report in several paragraphs.
What is the problem? For example, what are you trying to solve? Describe the motivation.
Why is this problem interesting? For example, is this problem helping us solve a bigger task in some way for society? Where would we find use cases for this problem in the community?
What is the approach you propose to tackle the problem? What approaches make sense for this problem? Would they work well or not?
What is the rationale behind the proposed approach? Did you find any reference for solving this problem previously? If there are, how does your approach differ from theirs (if any)?
What are the key components of the approach and results? Also, include any specific limitations.


The key problem is the difficulty in combating the sheer volume and speed with which misinformation spreads quickly online. Many users, particularly older adults and those with lower media literacy overall, struggle to tell real vs. fake stories. This is often exploited by fake headlines that use emotional or exaggerated language to attract clicks, likes, and viewership. The extremely negative effects of disinformation are relatively uncontroversial, including erosion of trust, increased polarization, and even political and social violence. Our approach here seeks to get at solving a bigger issue in society, which is the fact that the truth has become so objective to one's beliefs, and we often struggle to arrive at mainstream truths. As a society, when we are unable to agree on an objective truth—a foundation—it makes it very tough to agree on anything else. Furthermore, AI is often discussed through the lenses of its economic and technological benefits as well as its social detriments, but few understand or believe that it can have a genuinely positive effect on society. Our project seeks to highlight this possibility.

We are proposing using a retrieval-augmented classification system built around the ReAct (Thought -> Action -> Observation) loops to try to classify news headlines as either fake or true. This architecture uses a retrieval-augmented reasoning agent with a TF-IDF search method paired with a small LLM. The goal is to classify news titles automatically, while also providing some sort of reasoning and/or logic to the model's response to help the user understand the context to our agent's answer. The rationale was to leverage the interpretable step-by-step reasoning capabilities of an LLM while grounding its decisions with evidence retrieved via TF-IDF search. While the TF-IDF approach to classifying misinformation actually seems to be relatively common from research, our unique contribution was integrating this common approach into an LLM-based agent pipeline, which allows for complex decision logic via the ReAct schema that a simple TF-IDF algorithm may not be able to achieve. We also decided to allow the model to abstain from prediction in order to improve reliability by avoiding low-confidence guesses. While this decision may impact the usefulness and final performance of the model, we felt it was important to not "box the model in" to making a decision, and give it the breathing room to abstain from a choice. The key technical components of the model the small instruction-tuned LLM (Qwen 0.5B/1.5B) and the TF-IDF backend. Our experimental results show an accuracy of 46.77% on valid predictions (i.e., the ones the model doesn't abstain from). We found the project was hindered by a few specific limitations. A major limitation encountered was the sensitivity to prompt format and the general limited model capacity of the small LLM used.

Approach: 

The overall prediction process is governed by a Retrieval-Augmented classification algorithm, which is executed via a ReAct loop that's run by the small instruction-tuned Qwen 0.5B/1.5B LLM. When presented with a new news title that needs classification (fake/true), the LLM first generates a Thought (its reasoning) and then an Action, which is always a call to our custom search tool. This search tool is the TF-IDF backend; it then converts the input title into a vector using TF-IDF vectorization. From here, the search tool queries the corpus to retrieve the top k=5 most similar, already-labeled headlines using cosine-similarity ranking. These retrieved results are returned as the Observation and fed back into the LLM's context. The LLM then integrates this external evidence to generate its next Thought and decides to either continue the process for up to six steps or output the final classification (True, Fake, or Abstain). We make a few key assumptions and design choices in this approach. First, a key tenet of our project is the assumption that news titles actually contain enough information to make a determination as to whether the article is fake or true. This assumption is central to the project, as if this were not the case, more information (say the first few sentences of the article, for example) would be required for each input to generate a valid output. We also decided to allow the model to abstain from choosing fake or true when classifying a news model. This allows the model to ignore the situations where it is the most unsure (often likely the toughest calls), meaning the model will only make a choice when it is relatively sure of it's answer. This will help to improve the accuracy and trustworthiness of the agent, but also means that its utility will be lower given the many headlines it will not classify at all. Furthermore, for many missed headlines, the agent may have actually been correct to go with its intuition, but with an option to abstain, it may "chicken out" of answering a more difficult situation. Additional limitations include the model's capacity and our own compute power. Because we are using a relatively small LLM there is a limit to how much deep learning can happen in using the model in our agent pipeline. Furthermore, given we are working on home computers, many of us do not have on aggregate the computation for expansive testing or more intense learning & training. Lastly, our approach comes with a real sensitivity to the prompt format for the model; given the ReAct format we are using, the LLM must adhere to a strict syntax when parsing thoughts—resulting in possibly invalid or unparseable outputs.  

Experiments:

Our dataset is from WELFake and includes approximately 72,000 samples with a roughly equal binary label distribution (T/F) and news headlines covering a wide topical variety of subjects. The project uses a Retrieval-augmented agent pipeline. The LLM generates Thought/Action pairs, the TF-IDF backend performs the search, and the agent aggregates the tools to produce the final label. The model we use is a Qwen 0.5B/1.5B. We set our TF-IDF search depth to k=5, meaning that the TF-IDF search will pull the top 5 most similar (according to cosine similarity) results when searching for relevant, already-labelled headlines. We set new tokens a maximum of 120 to ensure efficiency and prevent hallucination by the LLM. Lastly, we include limit the model to 6 reasoning steps to ensure it arrives at an output reasonably, and also to prevent excessive rumination, whilst still allowing time for adequate reasoning and thought. 

Results:

From our experiment, out of 100 samples tested, only 62 were valid predictions (True/Fake), with 38 invalid/unparsed predictions. The accuracy of the 62 valid predictions was 46.77%. In our confusion matrix, we see the model correctly labeled 21 true headlines as true, mislabeled 7 true headlines as fake, mislabeled 26 fake headlines as true, and correctly labeled 8 fake headlines as fake. We further see that for the True class, the model had a precision of 0.45, a recall of 0.75, and an f1-score of 0.56. For the fake class, the agent had a precision of 0.53, a recall of 0.24, and f1-score of 0.33. These results show that while the model has a higher recall for true articles (0.75), it has very low precision (0.45), meaning it frequently misclassifies fake articles as true. Conversely, the recall for fake articles is very low (0.24), indicating it struggles significantly to identify misinformation.

Discussion:

From the results of our experiment, we clearly see that the model has struggled to effectively differentiate misinformation. In diagnosing the agent's shortcomings, we can draw a few conclusions. First, one of the biggest conclusions is that the headline is likely not enough information for the model to conclusively determine if an article if fake news or not. This is not enormously surprising, as many headlines are not immediately apparent to be true or false, and more information or context is often required. We can conclude additionally that the limited capacity of our LLM was likely a problem here, as the model was probably not strong enough to handle the complex ReAct reasoning process. Evidenced by the multitude of invalid or unparsed samples in our sample set, it's clear that the small model capacity had a serious effect here. Compute power was also an issue; running analysis on a broader or full data set was impossible because of the time required to classify each sample with the power of our home computers. The limited and statistically unsigificant data set we had to work with likely made our performance metrics somewhat skewed and not representative of the entire picture. Furthermore, we found that the mere presence of contradictory or misleading information (or "noise") online (of which there is a plethora of), would seriously confuse the LLM. Often, we could see from the reasoning chains that the model would be able to effectively conclude a true article is true (as there is a plethora of supporting information on the internet to confirm that), but would struggle with misinformation because there is usually at least some information (tweets, reddit threads, youtube videos, etc) that would support misinformation or at least confuse the model enough to prevent it from concluding something specific. As a result of this, the model is often more likely to abstain given the confusion it finds online. We did however find that in looking at the LLM's reasoning process, the thought processes were actually often on the right track. True headlines were very often properly classified and well identified, with sound logic behind them. However, false headlines would often introduce significant uncertainty into thought processes that would push the model to dive deeper for information, often finding results that caused even more confusion. 

In future iterations, a few changes could significantly improve the agent's performance. First, having access to the entire article would likely provide the LLM with significantly more context and hopefully improve its ability to draw definitive conclusions. This would hopefully help to reduce the poor recall score on true headlines and minimize the agent's reliance on noisy TF-IDF title retrievals. This would obviously require significantly more computational power to consume entire news articles rather than short few-word headlines. Increased model capacity and compute power could help to fix two large problems with our current approach. First, this might allow us to obtain a large enough sample to be statistically significant and produce more accurate and general performance metrics. We further would be able to employ a more powerful LLM that is less susceptible to edge case-induced confusion and more able to perform effectively in the ReAct structure. We could additionally replace our TF-IDF approach with a more complex search-and-retrieval method that may improve hte reliability and quality of the model's initial searches, rather than simply sharing keywords. 

Conclusion: In several sentences, summarize what you have achieved.


References: 

Shahane, Saurabh. “Fake News Classification.” Kaggle, 8 Oct. 2023, www.kaggle.com/datasets/saurabhshahane/fake-news-classification. 

