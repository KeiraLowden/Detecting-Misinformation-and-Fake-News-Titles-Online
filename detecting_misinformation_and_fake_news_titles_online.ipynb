{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 0: Enable GPU in Runtime Settings and Set up Python Execution Paths\n",
        "Click on the menu: Runtime → Change runtime type.\n",
        "\n",
        "Under Hardware accelerator, select: GPU (for NVIDIA GPUs like T4, P100, A100 depending on your Colab plan)\n",
        "\n",
        "Click “Save”.\n",
        "\n",
        "Now, your notebook will run with GPU acceleration.\n",
        "\n",
        "Next, let's verify GPU availability"
      ],
      "metadata": {
        "id": "C_xOuH9KAxgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "# If it returns True, GPU is active.\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmUCKl0EAvui",
        "outputId": "19fe0a25-7930-4b59-9d2d-0651d1f07459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Use this command to see which GPU you have\n",
        "# You will see the GPU version and the total availabel memory\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH7hQax7A5Ii",
        "outputId": "a9588829-5cc4-43f4-95a1-2470c3f87382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  8 22:33:01 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 0: Install required packages and libraries"
      ],
      "metadata": {
        "id": "vV1MY3wJDCPz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbYP7jl-2aad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6476a0d5-2e5f-4e4f-e48e-a30146a72c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn matplotlib seaborn nltk\n",
        "!pip install torch torchvision torchaudio transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtemGEJa2d8L",
        "outputId": "a21f6876-2fda-4ac1-bbe2-95fccf566a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# All libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix)\n",
        "import kagglehub\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Download and clean the data from KaggleHub for TF-IDF"
      ],
      "metadata": {
        "id": "C6MjAa6hCx4r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J62tdPFw3eNM",
        "outputId": "9506f0a7-05a7-43aa-f961-06538c8d5bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'fake-news-classification' dataset.\n",
            "                                               title  \\\n",
            "0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
            "1                                                NaN   \n",
            "2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
            "3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
            "4  SATAN 2: Russia unvelis an image of its terrif...   \n",
            "\n",
            "                                                text  label  \\\n",
            "0  No comment is expected from Barack Obama Membe...      1   \n",
            "1     Did they post their votes for Hillary already?      1   \n",
            "2   Now, most of the demonstrators gathered last ...      1   \n",
            "3  A dozen politically active pastors came here f...      0   \n",
            "4  The RS-28 Sarmat missile, dubbed Satan 2, will...      1   \n",
            "\n",
            "                                             content  \n",
            "0  law enforcement on high alert following threat...  \n",
            "1      did they post their votes for hillary already  \n",
            "2  unbelievable obamas attorney general says most...  \n",
            "3  bobby jindal raised hindu uses story of christ...  \n",
            "4  satan  russia unvelis an image of its terrifyi...  \n",
            "              label\n",
            "count  72134.000000\n",
            "mean       0.514404\n",
            "std        0.499796\n",
            "min        0.000000\n",
            "25%        0.000000\n",
            "50%        1.000000\n",
            "75%        1.000000\n",
            "max        1.000000\n"
          ]
        }
      ],
      "source": [
        "# download the latest version\n",
        "path = kagglehub.dataset_download(\"saurabhshahane/fake-news-classification\")\n",
        "\n",
        "# data_path = \"/kaggle/input/fake-news-classification/WELFake_Dataset.csv\"\n",
        "data_path = os.path.join(path, \"WELFake_Dataset.csv\")\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# drop unused col\n",
        "df = df.drop(columns=[\"Unnamed: 0\"])\n",
        "\n",
        "# combine the Title and text\n",
        "df['content'] = df['title'].fillna('') + \" \" + df['text'].fillna('')\n",
        "#make everything lowercase and remove spaces for TF-IDF search to happen more easily and accurately\n",
        "df['content'] = df['content'].str.lower().str.replace(r'[^a-z\\s]', '', regex = True)\n",
        "X = df['content']\n",
        "y = df['label']\n",
        "\n",
        "# show data for reference\n",
        "print(df.head())\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Split the data to train it"
      ],
      "metadata": {
        "id": "7lFy5qc_Eszk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siUTJ4Zp4Cgo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad3124e-e319-43be-88f9-4a81e90fcd29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training size: 57707\n",
            "Test size: 14427\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.2,      # 20% test set\n",
        "    random_state=42,    # ensures reproducibility\n",
        "    stratify=y          # keeps label proportions the same in train/test\n",
        ")\n",
        "\n",
        "print(\"Training size:\", len(X_train))\n",
        "print(\"Test size:\", len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Callable, Dict, List, Tuple, Optional, Any\n",
        "import json, math, re, textwrap, random, os, sys\n",
        "import math\n",
        "from collections import Counter, defaultdict\n"
      ],
      "metadata": {
        "id": "sgU9TzFcrV-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CORPUS = []\n",
        "\n",
        "for i, rows in df.iterrows():\n",
        "    title = rows[\"title\"]\n",
        "    text = rows[\"text\"]\n",
        "    label = rows[\"label\"]\n",
        "\n",
        "    if pd.isna(title):\n",
        "        title = \"\"\n",
        "    if pd.isna(text):\n",
        "        text = \"\"\n",
        "\n",
        "    CORPUS.append({\n",
        "        \"id\": str(i),\n",
        "        \"title\": str(title),\n",
        "        \"text\": str(text),\n",
        "        \"label\": int(rows[\"label\"])\n",
        "    })"
      ],
      "metadata": {
        "id": "0gaDZebfrWA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize(text: str) -> List[str]:\n",
        "    return re.findall(r\"[a-zA-Z0-9']+\", text.lower())\n",
        "\n",
        "DOC_TOKENS = [tokenize(d[\"title\"] + \" \" + d[\"text\"]) for d in CORPUS]\n",
        "\n",
        "VOCAB = sorted(set(t for doc in DOC_TOKENS for t in doc))\n",
        "\n",
        "def compute_tf(tokens: List[str]) -> Dict[str, float]:\n",
        "    # Input: A list of all the words in a document\n",
        "    # Output: A dictionary of the frequency of each word\n",
        "\n",
        "    doc_length = len(tokens)\n",
        "    term_counts = Counter(tokens)\n",
        "    tf_scores = {}\n",
        "    for term, count in term_counts.items():\n",
        "        tf_scores[term] = count / doc_length\n",
        "\n",
        "    return tf_scores\n",
        "\n"
      ],
      "metadata": {
        "id": "jQ2aojsuvPmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_df(doc_tokens: List[List[str]]) -> Dict[str, float]:\n",
        "    # Input: A list of lists of tokens in each document\n",
        "    # Output: A dictionary of the counts of each word appearing across the documents\n",
        "\n",
        "    df_counts = defaultdict(int)\n",
        "\n",
        "    for doc in doc_tokens:\n",
        "        unique_tokens = set(doc)\n",
        "        for token in unique_tokens:\n",
        "            df_counts[token] += 1\n",
        "\n",
        "    return dict(df_counts)"
      ],
      "metadata": {
        "id": "q8W_vlK9wXoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DF = compute_df(DOC_TOKENS) # Get the DF\n",
        "N_DOC = len(DOC_TOKENS) # number of docs\n",
        "IDF = {t: math.log((N_DOC + 1) / (DF[t] + 0.5)) + 1 for t in VOCAB} # Inverse document frequency\n",
        "\n",
        "def tfidf_vector(tokens: List[str]) -> Dict[str, float]:\n",
        "    # Input: A list of words in a document\n",
        "    # Output: A dictionary of tf-idf score of each word\n",
        "    tf = compute_tf(tokens)\n",
        "    vec = {t: tf[t] * IDF.get(t, 0.0) for t in tf}\n",
        "    return vec\n",
        "\n",
        "DOC_VECS = [tfidf_vector(tokens) for tokens in DOC_TOKENS]\n",
        "\n"
      ],
      "metadata": {
        "id": "sm7rDE60wWkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine(a: Dict[str, float], b: Dict[str, float]) -> float:\n",
        "    # Inputs: Two dictrionaries of tf-idf vectors of two document\n",
        "    # Output: The cosine similarity scalar between the two vector\n",
        "\n",
        "    if not a or not b:\n",
        "        return 0.0\n",
        "\n",
        "    all_terms = set(a.keys()) | set(b.keys())\n",
        "\n",
        "    dot_product = 0.0\n",
        "    norm_a_sq = 0.0\n",
        "    norm_b_sq = 0.0\n",
        "\n",
        "    for term in all_terms:\n",
        "        weight_a = a.get(term, 0.0)\n",
        "        weight_b = b.get(term, 0.0)\n",
        "\n",
        "        dot_product += weight_a * weight_b\n",
        "        norm_a_sq += weight_a ** 2\n",
        "        norm_b_sq += weight_b ** 2\n",
        "\n",
        "    denominator = math.sqrt(norm_a_sq) * math.sqrt(norm_b_sq)\n",
        "\n",
        "    similarity = 0.0\n",
        "    if denominator > 0:\n",
        "        similarity = dot_product / denominator\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "Ir2zWb38wU6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarizer\n",
        "def summarize_title(doc_text: str) -> str:\n",
        "  sentences = doc_text.split('.')\n",
        "  if len(sentences) == 0:\n",
        "    return doc_text\n",
        "  else:\n",
        "    return sentences[0].strip() + '.'"
      ],
      "metadata": {
        "id": "i3bqcjva_dq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_corpus(query: str, k: int = 3) -> List[Dict[str, Any]]:\n",
        "    qvec = tfidf_vector(tokenize(query))\n",
        "    scored = [(cosine(qvec, v), i) for i, v in enumerate(DOC_VECS)]\n",
        "    scored.sort(reverse=True)\n",
        "\n",
        "    top_k = scored[:k]\n",
        "    max_score = top_k[0][0] if top_k else 0.0\n",
        "\n",
        "    results = []\n",
        "    for score, idx in scored[:k]:\n",
        "        d = CORPUS[idx].copy()\n",
        "        d[\"score\"] = float(score)\n",
        "        results.append(d)\n",
        "    return {\n",
        "        \"results\": results,\n",
        "        \"query\": query,\n",
        "        \"confidence\": float(max_score),\n",
        "        \"summary\": summarize_title(query)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "BzzmIbmawTxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tool_search(query: str, k: int = 3) -> Dict[str, Any]:\n",
        "    hits = search_corpus(query, k=k)\n",
        "    # Return a concise, citation-friendly payload\n",
        "    return {\n",
        "        \"tool\": \"search\",\n",
        "        \"query\": query,\n",
        "        \"confidence\": hits[\"confidence\"],\n",
        "        \"summary\": hits[\"summary\"],\n",
        "        \"results\": [\n",
        "            {\"id\": h[\"id\"], \"title\": h[\"title\"], \"score\": h[\"score\"], \"snippet\": h[\"text\"][:240] + (\"...\" if len(h[\"text\"]) > 240 else \"\")}\n",
        "            for h in hits[\"results\"]\n",
        "        ],\n",
        "    }\n",
        "\n",
        "TOOLS = {\n",
        "    \"search\": {\n",
        "        \"schema\": {\"query\": \"str\", \"k\": \"int? (default=3)\"},\n",
        "        \"fn\": tool_search\n",
        "    },\n",
        "    \"finish\": {\n",
        "        \"schema\": {\"answer\": \"str\"},\n",
        "        \"fn\": lambda answer: {\"tool\": \"finish\", \"answer\": answer}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "Fy3M9ZLIwSp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# milestone2: prompting"
      ],
      "metadata": {
        "id": "-0-527ZQKarb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "# Regex to match the action patterns\n",
        "ACTION_RE = re.compile(\n",
        "    r'^(search|finish)\\[(.*)\\]$'\n",
        ")\n",
        "\n",
        "\n",
        "def parse_action(line):\n",
        "    \"\"\"\n",
        "    Parse an action line of the form:\n",
        "        Action: search[query=\"...\", k=3]\n",
        "        Action: finish[answer=\"...\"]\n",
        "    OR with no 'Action:' prefix (our trajectory stores this way).\n",
        "\n",
        "    Returns:\n",
        "        {\"type\": <str>, \"args\": <dict>}   OR   None\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove optional prefix\n",
        "    if line.startswith(\"Action:\"):\n",
        "        line = line[len(\"Action:\"):].strip()\n",
        "\n",
        "    m = ACTION_RE.match(line)\n",
        "    if not m:\n",
        "        return None\n",
        "\n",
        "    action_type, inside = m.groups()\n",
        "\n",
        "    # Split args inside brackets\n",
        "    parts = [p.strip() for p in inside.split(\",\")]\n",
        "\n",
        "    args = {}\n",
        "\n",
        "    for p in parts:\n",
        "        if \"=\" not in p:\n",
        "            continue\n",
        "        key, value = p.split(\"=\", 1)\n",
        "        key = key.strip()\n",
        "        value = value.strip()\n",
        "\n",
        "        # Strip quotes \"...\"\n",
        "        if value.startswith('\"') and value.endswith('\"'):\n",
        "            value = value[1:-1]\n",
        "\n",
        "        # Convert integer if appropriate\n",
        "        if value.isdigit():\n",
        "            value = int(value)\n",
        "\n",
        "        args[key] = value\n",
        "\n",
        "    return {\"type\": action_type, \"args\": args}\n",
        "\n"
      ],
      "metadata": {
        "id": "7uex-eO5tkcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a misinformation-detection agent.\n",
        "You operate in a strict loop of:\n",
        "\n",
        "    Thought: <your reasoning>\n",
        "    Action: <a single tool call>\n",
        "\n",
        "You have exactly TWO valid actions:\n",
        "\n",
        "1. search[query=\"<text>\", k=<int>]\n",
        "   - MUST be called as:\n",
        "       Action: search[query=\"<some text>\", k=<integer>]\n",
        "   - MUST include both query=\"...\" and k=...\n",
        "   - MUST NOT use lists or multiple strings.\n",
        "   - VALID:\n",
        "       Action: search[query=\"satan 2 russia missile\", k=5]\n",
        "       Action: search[query=\"obama christmas trees\", k=3]\n",
        "   - INVALID (NEVER DO THESE):\n",
        "       search[\"a\",\"b\"]\n",
        "       search[\"...\", 10]\n",
        "       search(\"something\")\n",
        "       Action: Action: search[...]\n",
        "       Action: search[text=\"...\", k=5]\n",
        "\n",
        "2. finish[answer=\"<label>\"]\n",
        "   - This ENDS the interaction.\n",
        "   - The answer MUST be EXACTLY one of:\n",
        "       \"fake\"\n",
        "       \"true\"\n",
        "   - VALID:\n",
        "       Action: finish[answer=\"fake\"]\n",
        "       Action: finish[answer=\"true\"]\n",
        "   - INVALID:\n",
        "       Action: finish[answer=\"this is probably fake\"]\n",
        "       Action: finish[answer=\"No\"]\n",
        "       Action: finish[answer=\"True. This headline is real.\"]\n",
        "\n",
        "TASK:\n",
        "- You are given a news headline.\n",
        "- Use search[...] to retrieve evidence from the corpus.\n",
        "- Reason step by step in Thought: lines.\n",
        "- When you are confident, choose:\n",
        "    - \"fake\"  → the headline is misinformation / false\n",
        "    - \"true\"  → the headline is accurate / real\n",
        "- Then call EXACTLY ONE final action:\n",
        "    Action: finish[answer=\"fake\"]\n",
        "    OR\n",
        "    Action: finish[answer=\"true\"]\n",
        "\n",
        "FORMATTING RULES (MANDATORY):\n",
        "\n",
        "- At EVERY step you MUST output TWO lines in this order:\n",
        "    Thought: <1–3 sentences of reasoning>\n",
        "    Action: <ONE valid action as defined above>\n",
        "\n",
        "- NEVER output Action without a Thought line right before it.\n",
        "- NEVER put the tool call inside Thought: — only inside Action:.\n",
        "- NEVER output more than ONE Action per step.\n",
        "- NEVER output “Action: Action: ...”.\n",
        "\n",
        "EXAMPLE (FOLLOW THIS PATTERN):\n",
        "\n",
        "Thought: I should search the corpus for evidence about 'SATAN 2' and Russia.\n",
        "Action: search[query=\"satan 2 russian missile\", k=3]\n",
        "Observation: {...}\n",
        "\n",
        "Thought: The documents suggest this is misinformation, so it is fake.\n",
        "Action: finish[answer=\"fake\"]\n",
        "\n",
        "Always follow this Thought → Action pattern for each step.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from dataclasses import asdict\n",
        "\n",
        "def make_prompt(question, trajectory):\n",
        "    \"\"\"\n",
        "    Build a ReAct-style prompt from:\n",
        "      • the system instructions\n",
        "      • the user question\n",
        "      • the ongoing Thought → Action → Observation steps\n",
        "\n",
        "    NOTE: We DO NOT end with 'Thought:' here. The model itself must\n",
        "    generate 'Thought:' followed by its reasoning.\n",
        "    \"\"\"\n",
        "    lines = [SYSTEM_PROMPT]\n",
        "    lines.append(f\"User Question: {question}\\n\")\n",
        "\n",
        "    for step in trajectory:\n",
        "        step = asdict(step)\n",
        "        if step.get(\"thought\"):\n",
        "            lines.append(f\"Thought: {step['thought']}\")\n",
        "        if step.get(\"action\"):\n",
        "            # 'action' already includes \"Action: ...\"\n",
        "            lines.append(step[\"action\"])\n",
        "        if step.get(\"observation\"):\n",
        "            lines.append(f\"Observation: {step['observation']}\\n\")\n",
        "\n",
        "    lines.append(\"Next step:\")\n",
        "\n",
        "    # No trailing \"Thought:\" line here!\n",
        "    return \"\\n\".join(lines)\n"
      ],
      "metadata": {
        "id": "6nBb4Up0vMij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# milestone 3: language model (hugging face)"
      ],
      "metadata": {
        "id": "-9X5CLE7vOvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# language_model.py\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "class LLMWrapper:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        max_new_tokens=120,\n",
        "        temperature=0.2,\n",
        "        top_p=0.95\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Loads a small GPT-style model from Hugging Face.\n",
        "        Designed to output ReAct steps:\n",
        "            Thought: ...\n",
        "            Action: search[...] or Action: finish[...]\n",
        "        \"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        print(f\"Loading model: {model_name} on {self.device} ...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
        "\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "        self.temperature = temperature\n",
        "        self.top_p = top_p\n",
        "\n",
        "    def __call__(self, prompt: str) -> str:\n",
        "      # Tokenize\n",
        "      enc = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "      input_ids = enc.input_ids.to(self.device)\n",
        "\n",
        "    # Build a simple attention mask to avoid the warning\n",
        "      attention_mask = (input_ids != self.tokenizer.eos_token_id).to(self.device)\n",
        "\n",
        "    # Use deterministic decoding to reduce format drift\n",
        "      output = self.model.generate(\n",
        "          input_ids,\n",
        "          attention_mask=attention_mask,\n",
        "          max_new_tokens=self.max_new_tokens,\n",
        "          do_sample=False,          # turn off sampling\n",
        "          temperature=0.0,          # ignored when do_sample=False, but explicit\n",
        "          top_p=1.0,\n",
        "          pad_token_id=self.tokenizer.eos_token_id,\n",
        "      )\n",
        "\n",
        "      decoded = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "      return decoded[len(prompt):].strip()\n"
      ],
      "metadata": {
        "id": "KuaWXFdOvsD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple functional interface LLM(prompt)\n",
        "_model_instance = None\n",
        "\n",
        "def LLM(prompt: str) -> str:\n",
        "    global _model_instance\n",
        "    if _model_instance is None:\n",
        "        _model_instance = LLMWrapper()\n",
        "    return _model_instance(prompt)"
      ],
      "metadata": {
        "id": "LNq4hTjWwBgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#missing the example he gave in github, not sure how to add it"
      ],
      "metadata": {
        "id": "tIvClSW8wPVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, asdict\n",
        "from typing import Callable, Dict, List, Tuple, Optional, Any\n",
        "import json, re\n",
        "\n",
        "@dataclass\n",
        "class Step:\n",
        "    thought: str\n",
        "    action: str\n",
        "    observation: str\n",
        "\n",
        "@dataclass\n",
        "class AgentConfig:\n",
        "    max_steps: int = 6\n",
        "    allow_tools: Tuple[str, ...] = (\"search\",)\n",
        "    verbose: bool = True\n",
        "\n",
        "class ReActAgent:\n",
        "    def __init__(self, llm: Callable[[str], str], tools: Dict[str, Dict[str, Any]], config: AgentConfig | None=None):\n",
        "        self.llm = llm\n",
        "        self.tools = tools\n",
        "        self.config = config or AgentConfig()\n",
        "        self.trajectory: List[Step] = []\n",
        "\n",
        "    def run(self, user_query: str) -> Dict[str, Any]:\n",
        "        self.trajectory.clear()\n",
        "\n",
        "        for step_idx in range(self.config.max_steps):\n",
        "\n",
        "            # 1. Build prompt\n",
        "            prompt = make_prompt(user_query, self.trajectory)\n",
        "\n",
        "            # 2. Model inference\n",
        "            out = self.llm(prompt)\n",
        "\n",
        "            # Extract Thought (first one)\n",
        "            t_match = re.search(r\"Thought:\\s*(.*)\", out)\n",
        "            thought = t_match.group(1).strip() if t_match else \"(no thought)\"\n",
        "\n",
        "# Extract only the core action: search[...] or finish[...]\n",
        "            a_match = re.search(r\"Action:\\s*(search\\[.*?\\]|finish\\[.*?\\])\", out, re.DOTALL)\n",
        "\n",
        "            if a_match:\n",
        "              raw_action = a_match.group(1).strip()   # exactly search[...] or finish[...]\n",
        "              action_line = f\"Action: {raw_action}\"\n",
        "            else:\n",
        "              action_line = 'Action: finish[answer=\"(no action)\"]'\n",
        "\n",
        "\n",
        "            # 3. Parse action\n",
        "            parsed = parse_action(action_line)\n",
        "\n",
        "            if not parsed:\n",
        "                observation = \"Invalid action format. Stopping.\"\n",
        "                self.trajectory.append(Step(thought, action_line, observation))\n",
        "                break\n",
        "\n",
        "            name = parsed[\"type\"]\n",
        "            args = parsed[\"args\"]\n",
        "\n",
        "            if name == \"finish\":\n",
        "                observation = \"done\"\n",
        "                self.trajectory.append(Step(thought, action_line, observation))\n",
        "                break\n",
        "\n",
        "            if name not in self.config.allow_tools or name not in self.tools:\n",
        "                observation = f\"Action '{name}' not allowed.\"\n",
        "                self.trajectory.append(Step(thought, action_line, observation))\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                obs_payload = self.tools[name][\"fn\"](**args)\n",
        "                observation = json.dumps(obs_payload, ensure_ascii=False)\n",
        "            except Exception as e:\n",
        "                observation = f\"Tool error: {e}\"\n",
        "\n",
        "            self.trajectory.append(Step(thought, action_line, observation))\n",
        "\n",
        "        final_answer = None\n",
        "        for s in reversed(self.trajectory):\n",
        "            if s.action.startswith(\"Action: finish[\"):\n",
        "                m = re.search(r'answer=\"(.*)\"', s.action)\n",
        "                if m:\n",
        "                    final_answer = m.group(1)\n",
        "                    break\n",
        "\n",
        "        return {\n",
        "            \"question\": user_query,\n",
        "            \"final_answer\": final_answer,\n",
        "            \"steps\": [asdict(s) for s in self.trajectory]\n",
        "        }\n"
      ],
      "metadata": {
        "id": "cDsUa-Cmws0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Dict, List, Tuple, Optional, Any\n",
        "\n",
        "agent = ReActAgent(LLM, TOOLS, AgentConfig(max_steps=6, verbose=True))\n",
        "\n",
        "demo_q = \"SATAN 2: Russia unvelis an image of its terrifying rocket\"\n",
        "result = agent.run(demo_q)\n",
        "\n",
        "print(\"Question:\", result[\"question\"])\n",
        "print(\"\\nFinal Answer:\", result[\"final_answer\"])\n",
        "print(\"\\nTrajectory:\")\n",
        "for i, s in enumerate(result[\"steps\"], 1):\n",
        "    print(f\"\\nStep {i}\")\n",
        "    print(\"Thought:\", s[\"thought\"])\n",
        "    print(\"Action:\", s[\"action\"])\n",
        "    print(\"Observation:\", s[\"observation\"][:500] + (\"...\" if len(s[\"observation\"])>500 else \"\"))\n",
        "\n",
        "# ----------------------------\n",
        "# Tiny Evaluation Harness\n",
        "# ----------------------------\n",
        "GOLD = {\n",
        "    \"SATAN 2: Russia unvelis an image of its terrifying rocket\":\n",
        "        [\"true\"],\n",
        "    \"Did Obama ban Christmas trees\":\n",
        "        [\"fake\"],\n",
        "    \"The Oxford 2025 Word of the Year Is 'Rage Bait'\":\n",
        "        [\"true\"]\n",
        "}\n",
        "\n",
        "def normalize(s: str) -> str:\n",
        "    return (s or \"\").lower()\n",
        "\n",
        "def em_contains(pred: Optional[str], gold_phrases: List[str]) -> bool:\n",
        "    p = normalize(pred)\n",
        "    return any(any(g in p for g in (normalize(gp),)) for gp in gold_phrases)\n",
        "\n",
        "def evaluate(agent: ReActAgent, questions: List[str]) -> Dict[str, Any]:\n",
        "    rows = []\n",
        "    for q in questions:\n",
        "        out = agent.run(q)\n",
        "        pred = out[\"final_answer\"]\n",
        "        gold_phrases = GOLD.get(q, [])\n",
        "        ok = em_contains(pred, gold_phrases) if gold_phrases else (pred is not None)\n",
        "        rows.append({\"question\": q, \"pred\": pred, \"ok\": ok})\n",
        "    acc = sum(r[\"ok\"] for r in rows) / max(1, len(rows))\n",
        "    return {\"accuracy\": acc, \"rows\": rows}\n",
        "\n",
        "eval_out = evaluate(agent, list(GOLD.keys()))\n",
        "print(\"\\n\\nEvaluation (toy):\", eval_out[\"accuracy\"])\n",
        "for r in eval_out[\"rows\"]:\n",
        "    print(\"-\", r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHnhBPmznBWQ",
        "outputId": "f269135b-9876-439c-d040-c9966de29342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-1.5B-Instruct on cuda ...\n",
            "Question: SATAN 2: Russia unvelis an image of its terrifying rocket\n",
            "\n",
            "Final Answer: (no action)\n",
            "\n",
            "Trajectory:\n",
            "\n",
            "Step 1\n",
            "Thought: I need to find information on whether this claim about Satan 2 being related to Russia's missile program is true or not. It seems like there might be some confusion with the name 'Satan 2', which could refer to different things depending on context. However, if we assume that 'Satan 2' refers to a specific missile system, then searching for information about it would help clarify the situation. Action: search[query=\"satan 2 russia missile\", k=5\n",
            "Action: Action: search[query=\"satan 2 russia missile\", k=5]\n",
            "Observation: {\"tool\": \"search\", \"query\": \"satan 2 russia missile\", \"confidence\": 0.45877358004305635, \"summary\": \"satan 2 russia missile.\", \"results\": [{\"id\": \"13818\", \"title\": \"Why did Satan-2 shock the West?\", \"score\": 0.45877358004305635, \"snippet\": \"Why did Satan-2 shock the West? 31.10.2016 The recent publication of images of Russia's new intercontinental ballistic missile \\\" Sarmat \\\" created quite a stir in Western media. The Daily Mail, for example, terrified British readers with an ...\"}, {\"id\": \"37...\n",
            "\n",
            "Step 2\n",
            "Thought: From the search results, it appears that the Russian government has released images of their new intercontinental ballistic missile, known as Satan 2. These images show the missile's terrifying capabilities and potential destructive power. The headlines mention that these images were taken by Russia and reveal details about the weapon's design and range. Given that the claims made in the headlines align with what was revealed through the images, it suggests that the information provided is factual. Therefore, the headline can be considered accurate.\n",
            "Action: Action: finish[answer=\"(no action)\"]\n",
            "Observation: done\n",
            "\n",
            "\n",
            "Evaluation (toy): 0.3333333333333333\n",
            "- {'question': 'SATAN 2: Russia unvelis an image of its terrifying rocket', 'pred': '(no action)', 'ok': False}\n",
            "- {'question': 'Did Obama ban Christmas trees', 'pred': 'fake', 'ok': True}\n",
            "- {'question': \"The Oxford 2025 Word of the Year Is 'Rage Bait'\", 'pred': '(no action)', 'ok': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# In this dataset: label 1 = fake news, 0 = true news\n",
        "def pred_to_label(pred: str) -> int | None:\n",
        "    \"\"\"\n",
        "    Map the agent's final_answer string to numeric label.\n",
        "    Returns:\n",
        "        0 for 'fake', 1 for 'true', None if can't interpret.\n",
        "    \"\"\"\n",
        "    if pred is None:\n",
        "        return None\n",
        "    p = str(pred).lower()\n",
        "    if \"fake\" in p:\n",
        "        return 0\n",
        "    if \"true\" in p:\n",
        "        return 1\n",
        "    return None\n",
        "\n",
        "# Sample up to 100 observations from the dataframe\n",
        "N_SAMPLE = 100\n",
        "sample_df = df.sample(n=min(N_SAMPLE, len(df)), random_state=7)\n",
        "\n",
        "gold_labels = []\n",
        "pred_labels = []\n",
        "questions = []\n",
        "raw_answers = []\n",
        "\n",
        "print(f\"Running agent on {len(sample_df)} headlines...\")\n",
        "for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
        "\n",
        "    # Use title as the \"headline question\" fall back to content if needed\n",
        "    if pd.notna(row.get(\"title\")) and str(row[\"title\"]).strip():\n",
        "        q = str(row[\"title\"])\n",
        "    else:\n",
        "        q = str(row.get(\"content\", \"\"))\n",
        "\n",
        "    out = agent.run(q)\n",
        "    pred_str = out.get(\"final_answer\", None)\n",
        "\n",
        "    pred_lbl = pred_to_label(pred_str)  # 1, 0, or None\n",
        "\n",
        "    questions.append(q)\n",
        "    raw_answers.append(pred_str)\n",
        "    gold_labels.append(int(row[\"label\"]))\n",
        "    pred_labels.append(pred_lbl)\n",
        "\n",
        "gold_labels = np.array(gold_labels)\n",
        "pred_labels = np.array(pred_labels, dtype=object)\n",
        "\n",
        "# How many predictions were usable (true/fake) vs None?\n",
        "valid_mask = np.array([p in (0, 1) for p in pred_labels])\n",
        "n_valid = valid_mask.sum()\n",
        "n_total = len(pred_labels)\n",
        "print(f\"\\nValid predictions (true/fake): {n_valid}/{n_total}\")\n",
        "print(f\"Invalid / unparsed predictions: {n_total - n_valid}\")\n",
        "\n",
        "if n_valid == 0:\n",
        "    print(\"No valid predictions to evaluate. Check the agent's output format.\")\n",
        "else:\n",
        "    # For evaluation, ignore rows where the model gave no interpretable label\n",
        "    y_true = gold_labels[valid_mask]\n",
        "    y_pred = pred_labels[valid_mask].astype(int)\n",
        "\n",
        "    # Overall accuracy on the valid subset\n",
        "    acc = (y_true == y_pred).mean()\n",
        "    print(f\"\\nAccuracy on {n_valid} valid predictions: {acc:.4f}\")\n",
        "\n",
        "    # Confusion matrix: rows = true (1=true, 0=fake), cols = pred\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
        "    print(\"\\nConfusion matrix (rows=true, cols=pred; 0=true, 1=fake):\")\n",
        "    print(cm)\n",
        "\n",
        "    # detailed classification report\n",
        "    print(\"\\nClassification report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=[\"true\", \"fake\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "a744526b910641188ef4dd6ff18908eb",
            "1833533403c4447193baf1dfbb75f32c",
            "777e6aea7e9348a297874e393c68ed37",
            "ca6fb2c79b694907b33d171fab6c3656",
            "e6ff3eda9965443583099e7fc53e9f3c",
            "67482609d1f745a38363eb231c1d4386",
            "915611e747704d7591a4498e49064d9e",
            "43758107a5a94bf7a22f3d801cee7209",
            "1461cd21685043c6aa96fdce8985920d",
            "f4b8d2cecc504c47a2e477b78fd34e91",
            "49fae9fbff8a4ad6bb5337f7a5141764"
          ]
        },
        "id": "TeWzK8A-7oYP",
        "outputId": "ce6e0894-8066-47f7-b6d4-05cf79bac426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent on 100 headlines...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a744526b910641188ef4dd6ff18908eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Valid predictions (true/fake): 62/100\n",
            "Invalid / unparsed predictions: 38\n",
            "\n",
            "Accuracy on 62 valid predictions: 0.4677\n",
            "\n",
            "Confusion matrix (rows=true, cols=pred; 0=true, 1=fake):\n",
            "[[21  7]\n",
            " [26  8]]\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        true       0.45      0.75      0.56        28\n",
            "        fake       0.53      0.24      0.33        34\n",
            "\n",
            "    accuracy                           0.47        62\n",
            "   macro avg       0.49      0.49      0.44        62\n",
            "weighted avg       0.49      0.47      0.43        62\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "A4cccvbf7wFG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a744526b910641188ef4dd6ff18908eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1833533403c4447193baf1dfbb75f32c",
              "IPY_MODEL_777e6aea7e9348a297874e393c68ed37",
              "IPY_MODEL_ca6fb2c79b694907b33d171fab6c3656"
            ],
            "layout": "IPY_MODEL_e6ff3eda9965443583099e7fc53e9f3c"
          }
        },
        "1833533403c4447193baf1dfbb75f32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67482609d1f745a38363eb231c1d4386",
            "placeholder": "​",
            "style": "IPY_MODEL_915611e747704d7591a4498e49064d9e",
            "value": "100%"
          }
        },
        "777e6aea7e9348a297874e393c68ed37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43758107a5a94bf7a22f3d801cee7209",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1461cd21685043c6aa96fdce8985920d",
            "value": 100
          }
        },
        "ca6fb2c79b694907b33d171fab6c3656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4b8d2cecc504c47a2e477b78fd34e91",
            "placeholder": "​",
            "style": "IPY_MODEL_49fae9fbff8a4ad6bb5337f7a5141764",
            "value": " 100/100 [23:29&lt;00:00,  6.64s/it]"
          }
        },
        "e6ff3eda9965443583099e7fc53e9f3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67482609d1f745a38363eb231c1d4386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "915611e747704d7591a4498e49064d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43758107a5a94bf7a22f3d801cee7209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1461cd21685043c6aa96fdce8985920d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4b8d2cecc504c47a2e477b78fd34e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49fae9fbff8a4ad6bb5337f7a5141764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}